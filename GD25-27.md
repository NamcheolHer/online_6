<p><a target="_blank" href="https://app.eraser.io/workspace/aJvSKAFc25fpWQxaIrT7" id="edit-in-eraser-github-link"><img alt="Edit in Eraser" src="https://firebasestorage.googleapis.com/v0/b/second-petal-295822.appspot.com/o/images%2Fgithub%2FOpen%20in%20Eraser.svg?alt=media&amp;token=968381c8-a7e7-472a-8ed6-4a6626da5501"></a></p>

## 25
- **내용이 상당히 많습니다.( 읽고 생각해볼 거리.)**




## 26 부터 합니다.
RLHF 

- SFT
- RM
- PPO


### 방식
- 각 스텝 이끔이 선정
- 이끔이와 키워드,** 퀴즈**, 참고자료 정리 및 토론하며 진행
- 이끔이 역할 : 스텝학습시간 정하기, 주제 진행
- 키워드
### 26-1_ 박태하
- 진행 시각
    - ~11:00: 퀴즈1번_답) GPT -> GPT와 버트의 차이를 알아야함. 
    - [﻿ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/](https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/) 
    - Gpt: [﻿https://drive.google.com/file/d/1FniCAiE4dxK-l2oVfX3iiuqGXO-BnmyO/view?usp=sharing](https://drive.google.com/file/d/1FniCAiE4dxK-l2oVfX3iiuqGXO-BnmyO/view?usp=sharing)  
    - bert : [﻿https://drive.google.com/file/d/1uVgpf4Ul7u17P9j1MfEhxGJNp6IJO60Z/view?usp=sharing](https://drive.google.com/file/d/1uVgpf4Ul7u17P9j1MfEhxGJNp6IJO60Z/view?usp=sharing) 
    - ~11:25: 퀴즈2번_답) RLHF인 것 같은데, 구체적인 구조는 노드 밀면서 이해해 봐야할 듯.
    - ~11:50 KoChatGPT 영상(12:18-28:53) 보고 이야기
- 키워드 : 디코더 모델, BPE
- 참고자료
12:18부터(또는 처음부터) 28:53 구간

### 26-2_김서연
- 진행 시각: 
    - ~12:15: Q4까지
    - ~12:45: Q8까지
- 키워드 :  
    - 그리디 서치 디코딩 
        - 그리디 서치(Greedy Search) 디코딩은 자연어 처리 작업에서 쓰이는 디코딩 방법 중 하나입니다. 이 방법은 각 단계에서 가장 높은 확률을 가진 다음 토큰을 선택하는 방식으로, 주요 특징과 과정은 다음과 같습니다:
            1. **단계별 최대 확률 선택**: 모델은 각 시간 단계에서 가능한 토큰 중 가장 높은 확률을 가진 토큰을 선택
            2. **순차적 진행**: 선택된 토큰은 다음 단계의 디코딩 입력으로 쓰이며, 모델은 이에 기반하여 다음 토큰 예측
            3. **종료 조건**: 종료 토큰이 나타나거나, 설정된 최대 길이에 도달할 때까지 반복
        - 장점: 계산이 효율적이며 간단
        - 단점: 최적의 결과를 항상 보장하지는 않음(확률이 높은 토큰을 선택한 결과 전체 문장 흐름이 악화될 수 있음)
            - 예를 들어, "The weather is" 다음에 올 수 있는 토큰들 중에서 "sunny"가 가장 확률이 높다면, 그리디 서치는 "sunny"를 선택합니다. 이후에도 모델은 계속해서 각 단계에서 가장 확률이 높은 토큰을 선택하여 문장을 완성합니다.
            - 그리디 서치의 단점을 극복하기 위해 극복하기 위해 빔 서치(Beam Search)나 다른 고급 디코딩 기법이 제안됨
    - 빔 서치 디코딩
        - 빔 서치는 가능한 선택지 중 최적 또는 더 나은 결과를 찾기 위해 사용되며, 그리디 서치보다 더 정교한 것으로 평가됩니다. 이 방법의 핵심 개념과 과정은 다음과 같습니다:
            1. **빔 크기(Beam Size)**: 
                - '빔 크기'라고 불리는 파라미터를 사용하여 각 디코딩 단계에서 고려할 후보 시퀀스의 수 결정
                - 예를 들어, 빔 크기가 3이면 각 단계에서 상위 3개의 가장 가능성 있는 시퀀스를 유지
            2. **후보 시퀀스 확장**:
                - 각 디코딩 단계에서, 현재까지의 최상위 후보 시퀀스들은 다음 단계에 가능한 모든 토큰으로 확장
                - 이렇게 생성된 새로운 시퀀스를 평가하여 다음 단계에서 고려할 최상위 후보 선정
            3. **평가 및 선택**:
                - 확률에 기반하여 평가된 새 시퀀스들 중 가장 높은 확률을 가진 시퀀스를 다음 단계로 전달
                - 그리디 서치와 마찬가지로 특정 종료 조건(예: 최대 길이 도달, 종료 토큰 등장)이 충족될 때까지 반복
            4. **최종 선택**:
                - 디코딩이 완료되면, 가장 높은 총 확률을 가진 시퀀스가 최종 출력으로 선택
        - 장점: 빔 서치는 여러 가능성 있는 경로를 동시에 고려함으로써, 단순히 각 단계에서 가장 확률이 높은 토큰을 선택하는 그리디서치보다 나은 결과를 얻을 수 있습니다. 
        - 단점: 빔 크기가 커질수록 계산 복잡성이 증가하고, 모든 가능한 조합을 고려하지는 않기 때문에 여전히 최적의 해를 보장하지는 않습니다. 
        - 빔 서치는 특히 긴 시퀀스를 생성하는 작업에서 유리하며, 더 적절하고 자연스러운 결과를 도출할 수 있습니다.
        - `model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2, do_sample=True, temperature=2.0, top_k=50)` 
            - `**num_beams**` 
빔의 수를 나타냅니다. 위 코드에서 `**num_beams=7**` 로 설정되어 있으므로, 7개의 다른 가능한 단어 시퀀스를 동시에 유지하면서 텍스트를 생성하려고 시도합니다. 이것은 다양한 후보 단어 시퀀스를 유지하여 다양한 결과를 생성하려는 것을 의미합니다. 그런 다음 이러한 후보 중에서 가장 가능성 있는 텍스트 시퀀스를 선택합니다.
            - `**temperature**` 
확률 분포를 조절하는 파라미터입니다. 높은 온도(예: 2.0)는 모델이 더 다양한 선택을 하게 하고, 낮은 온도는 모델이 더 확신 있는 선택을 하게 합니다. 즉, 높은 온도는 더 다양한 텍스트를 생성할 가능성이 높고, 낮은 온도는 보다 확고한 예측을 할 가능성이 높습니다. 높은 온도는 더 랜덤한 결과를 가져올 수 있으며, 낮은 온도는 더 확실한 결과를 가져올 수 있습니다.
            - `**top_k**` 
모델이 고려할 가능한 다음 단어의 후보를 제한하는 파라미터입니다. `**top_k**`  값은 가능한 다음 단어 중에서 가장 높은 확률을 가진 상위 단어들의 수를 나타냅니다. 예를 들어, `**top_k=50**` 으로 설정하면 모델은 다음 단어 후보 중 상위 50개만 고려하고, 나머지는 무시합니다. 이것은 텍스트 생성 과정에서 가능한 선택의 다양성을 조절하는 데 사용됩니다.
            - ****`**top-p**` 
top-p 샘플링은 모델이 다음 단어를 선택할 때 누적 분포를 기반으로 확률이 높은 상위 단어를 선택하는 방식을 채택합니다.이 방식은 모델이 상위 확률을 가진 단어를 주로 선택하면서도 조금의 다양성을 유지하도록 도와줍니다. 이는 항상 가장 확률이 높은 단어를 선택하는 'argmax' 방식보다 다양한 텍스트를 생성하는 데 도움이 됩니다. 
                1. 먼저, 모델은 현재까지 생성된 텍스트와 관련하여 가능한 모든 다음 단어의 확률을 계산합니다.
                2. 그런 다음, 이러한 확률을 크기순으로 정렬합니다.
                3. 이제 누적 확률 분포를 생성합니다. 이 분포는 확률이 가장 높은 단어부터 시작하여 순서대로 더해지는데, 이 때 미리 정의된 임계값(일반적으로 `top_p=0.8` 또는 `0.9`)이내로 확률범위를 제한한다.
                4. 마지막으로, 누적 확률 분포를 기반으로 임의로 단어를 선택합니다. 이때, 누적 확률 분포 내에서 선택된 단어의 위치가 무작위성을 가지며, 확률이 높은 단어일수록 선택될 확률이 더 높습니다.
- 퀴즈
1. temperature 는 확률 분포를 조절하는 파라미터입니다. 높은 온도(예: 2.0)는 모델이 더 다양한 선택을 하게 하고, 낮은 온도는 모델이 더 확신 있는 선택을 하게 합니다. 즉, 높은 온도는 더 다양한 텍스트를 생성할 가능성이 높고, 낮은 온도는 보다 확고한 예측을 할 가능성이 높습니다. 높은 온도는 더 랜덤한 결과를 가져올 수 있으며, 낮은 온도는 더 확실한 결과를 가져올 수 있습니다.
top_k 는 모델이 고려할 가능한 다음 단어의 후보를 제한하는 파라미터입니다. top_k  값은 가능한 다음 단어 중에서 가장 높은 확률을 가진 상위 단어들의 수를 나타냅니다. 
2. top-p 샘플링은 모델이 다음 단어를 선택할 때 누적 분포를 기반으로 확률이 높은 상위 단어를 선택하는 방식을 채택합니다.이 방식은 모델이 상위 확률을 가진 단어를 주로 선택하면서도 조금의 다양성을 유지하도록 도와줍니다. 이는 항상 가장 확률이 높은 단어를 선택하는 'argmax' 방식보다 다양한 텍스트를 생성하는 데 도움이 됩니다. 
3. 지시문에 응답하지 않음. 지시문에서 이어지는 텍스트를 출력.
4. 한국어 QA 데이터셋의 질문으로 답변 자동 생성(ChatGPT), langchain을 이용한 채팅데이터 자동생성(ChatGPT)
5. Ranking 데이터가 필요하므로 동일한 prompt에 대해 각기 다른 3가지 답변 자동 생성 (ChatGPT로 생성, GPT3로 생성('text-davinci-003'), GPT3로 생성('text-ada-001')). 이후, ChatGPT > GPT3-davinci > GPT3-ada 순으로 랜덤하게 섞은 후 ranking 자동 생성.
6. 
- 참고자료
    - 디코딩 전략: [﻿sooftware.io/generate/](https://sooftware.io/generate/) 
    - top-p 샘플링: [﻿velog.io/@nawnoes/Top-p-%EC%83%98%ED%94%8C%EB%A7%81-aka.-Nucleus-Sampling](https://velog.io/@nawnoes/Top-p-%EC%83%98%ED%94%8C%EB%A7%81-aka.-Nucleus-Sampling) 
### 26-3_임정훈
- 진행 시각:
- 키워드 : 
- 퀴즈
1. 
2. 
- 참고자료


### 26-4_서민성
- 진행 시각:
- 키워드 : 
- 퀴즈
1. 
2. 
- 참고자료


### 26-5_김지원
- 진행 시각:
- 키워드 : 
- 퀴즈
1. 
2. 
- 참고자료


### 26-6_김정현
- 진행 시각:
- 키워드 : 
- 퀴즈
1. 
2. 
- 참고자료


## 27 프로젝트 



<!--- Eraser file: https://app.eraser.io/workspace/aJvSKAFc25fpWQxaIrT7 --->