<p><a target="_blank" href="https://app.eraser.io/workspace/aJvSKAFc25fpWQxaIrT7" id="edit-in-eraser-github-link"><img alt="Edit in Eraser" src="https://firebasestorage.googleapis.com/v0/b/second-petal-295822.appspot.com/o/images%2Fgithub%2FOpen%20in%20Eraser.svg?alt=media&amp;token=968381c8-a7e7-472a-8ed6-4a6626da5501"></a></p>

## 진행방식
- 25번 노드는 **내용이 상당히 많아 추후 학습자료로 남겨두고, **26번 노드를 오늘 하루 진행합니다.
- 각 스텝 이끔이 선정, 이끔이와 키워드,** 퀴즈**, 참고자료 정리 및 토론하며 진행
- 이끔이 역할 : 스텝학습시간 정하기, 주제 진행
- 키워드
## 주요 개념
### RLHF 
- SFT (Supervised FineTtuning)
- RM (Reward Model)
- PPO (Proximal Policy Optimization - one of Reinforcement Learning algorithms)


## 26-1_ 박태하
- ~11:00: 퀴즈1번_답) GPT -> GPT와 버트의 차이를 알아야함. 
- [﻿ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/](https://ratsgo.github.io/nlpbook/docs/language_model/bert_gpt/) 
- Gpt: [﻿https://drive.google.com/file/d/1FniCAiE4dxK-l2oVfX3iiuqGXO-BnmyO/view?usp=sharing](https://drive.google.com/file/d/1FniCAiE4dxK-l2oVfX3iiuqGXO-BnmyO/view?usp=sharing)  
- bert : [﻿https://drive.google.com/file/d/1uVgpf4Ul7u17P9j1MfEhxGJNp6IJO60Z/view?usp=sharing](https://drive.google.com/file/d/1uVgpf4Ul7u17P9j1MfEhxGJNp6IJO60Z/view?usp=sharing) 
- ~11:25: 퀴즈2번_답) RLHF인 것 같은데, 구체적인 구조는 노드 밀면서 이해해 봐야할 듯.
- ~11:50 KoChatGPT 영상(12:18-28:53) 보고 이야기
- 키워드 : 디코더 모델, BPE
- 참고자료
12:18부터(또는 처음부터) 28:53 구간

## 26-2_김서연
### 진행 시각
- ~12:15: Q4까지
- ~12:45: Q8까지
### 키워드
#### 그리드 서치 디코딩
- 그리디 서치(Greedy Search) 디코딩은 자연어 처리 작업에서 쓰이는 디코딩 방법 중 하나입니다. 이 방법은 각 단계에서 가장 높은 확률을 가진 다음 토큰을 선택하는 방식으로, 주요 특징과 과정은 다음과 같습니다:
    1. **단계별 최대 확률 선택**: 모델은 각 시간 단계에서 가능한 토큰 중 가장 높은 확률을 가진 토큰을 선택
    2. **순차적 진행**: 선택된 토큰은 다음 단계의 디코딩 입력으로 쓰이며, 모델은 이에 기반하여 다음 토큰 예측
    3. **종료 조건**: 종료 토큰이 나타나거나, 설정된 최대 길이에 도달할 때까지 반복
- 장점: 계산이 효율적이며 간단
- 단점: 최적의 결과를 항상 보장하지는 않음(확률이 높은 토큰을 선택한 결과 전체 문장 흐름이 악화될 수 있음)
    - 예를 들어, "The weather is" 다음에 올 수 있는 토큰들 중에서 "sunny"가 가장 확률이 높다면, 그리디 서치는 "sunny"를 선택합니다. 이후에도 모델은 계속해서 각 단계에서 가장 확률이 높은 토큰을 선택하여 문장을 완성합니다.
    - 그리디 서치의 단점을 극복하기 위해 극복하기 위해 빔 서치(Beam Search)나 다른 고급 디코딩 기법이 제안됨
#### 빔 서치 디코딩
- 빔 서치는 가능한 선택지 중 최적 또는 더 나은 결과를 찾기 위해 사용되며, 그리디 서치보다 더 정교한 것으로 평가됩니다. 이 방법의 핵심 개념과 과정은 다음과 같습니다:
    1. **빔 크기(Beam Size)**: 
        - '빔 크기'라고 불리는 파라미터를 사용하여 각 디코딩 단계에서 고려할 후보 시퀀스의 수 결정
        - 예를 들어, 빔 크기가 3이면 각 단계에서 상위 3개의 가장 가능성 있는 시퀀스를 유지
    2. **후보 시퀀스 확장**:ㅔ
        - 각 디코딩 단계에서, 현재까지의 최상위 후보 시퀀스들은 다음 단계에 가능한 모든 토큰으로 확장
        - 이렇게 생성된 새로운 시퀀스를 평가하여 다음 단계에서 고려할 최상위 후보 선정
    3. **평가 및 선택**:
        - 확률에 기반하여 평가된 새 시퀀스들 중 가장 높은 확률을 가진 시퀀스를 다음 단계로 전달
        - 그리디 서치와 마찬가지로 특정 종료 조건(예: 최대 길이 도달, 종료 토큰 등장)이 충족될 때까지 반복
    4. **최종 선택**:
        - 디코딩이 완료되면, 가장 높은 총 확률을 가진 시퀀스가 최종 출력으로 선택
- 장점: 빔 서치는 여러 가능성 있는 경로를 동시에 고려함으로써, 단순히 각 단계에서 가장 확률이 높은 토큰을 선택하는 그리디서치보다 나은 결과를 얻을 수 있습니다. 
- 단점: 빔 크기가 커질수록 계산 복잡성이 증가하고, 모든 가능한 조합을 고려하지는 않기 때문에 여전히 최적의 해를 보장하지는 않습니다. 
- 빔 서치는 특히 긴 시퀀스를 생성하는 작업에서 유리하며, 더 적절하고 자연스러운 결과를 도출할 수 있습니다.
- `model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2, do_sample=True, temperature=2.0, top_k=50)` 
    - `**num_beams**` 
빔의 수를 나타냅니다. 위 코드에서 `**num_beams=7**` 로 설정되어 있으므로, 7개의 다른 가능한 단어 시퀀스를 동시에 유지하면서 텍스트를 생성하려고 시도합니다. 이것은 다양한 후보 단어 시퀀스를 유지하여 다양한 결과를 생성하려는 것을 의미합니다. 그런 다음 이러한 후보 중에서 가장 가능성 있는 텍스트 시퀀스를 선택합니다.
    - `**temperature**` 
확률 분포를 조절하는 파라미터입니다. 높은 온도(예: 2.0)는 모델이 더 다양한 선택을 하게 하고, 낮은 온도는 모델이 더 확신 있는 선택을 하게 합니다. 즉, 높은 온도는 더 다양한 텍스트를 생성할 가능성이 높고, 낮은 온도는 보다 확고한 예측을 할 가능성이 높습니다. 높은 온도는 더 랜덤한 결과를 가져올 수 있으며, 낮은 온도는 더 확실한 결과를 가져올 수 있습니다.
    - `**top_k**` 
모델이 고려할 가능한 다음 단어의 후보를 제한하는 파라미터입니다. `**top_k**`  값은 가능한 다음 단어 중에서 가장 높은 확률을 가진 상위 단어들의 수를 나타냅니다. 예를 들어, `**top_k=50**` 으로 설정하면 모델은 다음 단어 후보 중 상위 50개만 고려하고, 나머지는 무시합니다. 이것은 텍스트 생성 과정에서 가능한 선택의 다양성을 조절하는 데 사용됩니다.
    - ****`**top-p**` 
top-p 샘플링은 모델이 다음 단어를 선택할 때 누적 분포를 기반으로 확률이 높은 상위 단어를 선택하는 방식을 채택합니다.이 방식은 모델이 상위 확률을 가진 단어를 주로 선택하면서도 조금의 다양성을 유지하도록 도와줍니다. 이는 항상 가장 확률이 높은 단어를 선택하는 'argmax' 방식보다 다양한 텍스트를 생성하는 데 도움이 됩니다. 
        1. 먼저, 모델은 현재까지 생성된 텍스트와 관련하여 가능한 모든 다음 단어의 확률을 계산합니다.
        2. 그런 다음, 이러한 확률을 크기순으로 정렬합니다.
        3. 이제 누적 확률 분포를 생성합니다. 이 분포는 확률이 가장 높은 단어부터 시작하여 순서대로 더해지는데, 이 때 미리 정의된 임계값(일반적으로 `top_p=0.8` 또는 `0.9`)이내로 확률범위를 제한한다.
        4. 마지막으로, 누적 확률 분포를 기반으로 임의로 단어를 선택합니다. 이때, 누적 확률 분포 내에서 선택된 단어의 위치가 무작위성을 가지며, 확률이 높은 단어일수록 선택될 확률이 더 높습니다.
### 퀴즈
1. temperature 는 확률 분포를 조절하는 파라미터입니다. 높은 온도(예: 2.0)는 모델이 더 다양한 선택을 하게 하고, 낮은 온도는 모델이 더 확신 있는 선택을 하게 합니다. 즉, 높은 온도는 더 다양한 텍스트를 생성할 가능성이 높고, 낮은 온도는 보다 확고한 예측을 할 가능성이 높습니다. 높은 온도는 더 랜덤한 결과를 가져올 수 있으며, 낮은 온도는 더 확실한 결과를 가져올 수 있습니다.
top_k 는 모델이 고려할 가능한 다음 단어의 후보를 제한하는 파라미터입니다. top_k  값은 가능한 다음 단어 중에서 가장 높은 확률을 가진 상위 단어들의 수를 나타냅니다. 
2. top-p 샘플링은 모델이 다음 단어를 선택할 때 누적 분포를 기반으로 확률이 높은 상위 단어를 선택하는 방식을 채택합니다.이 방식은 모델이 상위 확률을 가진 단어를 주로 선택하면서도 조금의 다양성을 유지하도록 도와줍니다. 이는 항상 가장 확률이 높은 단어를 선택하는 'argmax' 방식보다 다양한 텍스트를 생성하는 데 도움이 됩니다. 
3. 지시문에 응답하지 않음. 지시문에서 이어지는 텍스트를 출력.
4. 한국어 QA 데이터셋의 질문으로 답변 자동 생성(ChatGPT), langchain을 이용한 채팅데이터 자동생성(ChatGPT)
5. Ranking 데이터가 필요하므로 동일한 prompt에 대해 각기 다른 3가지 답변 자동 생성 (ChatGPT로 생성, GPT3로 생성('text-davinci-003'), GPT3로 생성('text-ada-001')). 이후, ChatGPT > GPT3-davinci > GPT3-ada 순으로 랜덤하게 섞은 후 ranking 자동 생성. 사람이 labeling 시 문장을 읽고 ranking을 0~2로 순위를 매긴다
6. SFT 데이터셋에서 prompt만 가져와서 jsonl 형태로 변형후 저장
- 참고자료
    - 디코딩 전략: [﻿sooftware.io/generate/](https://sooftware.io/generate/) 
    - top-p 샘플링: [﻿velog.io/@nawnoes/Top-p-%EC%83%98%ED%94%8C%EB%A7%81-aka.-Nucleus-Sampling](https://velog.io/@nawnoes/Top-p-%EC%83%98%ED%94%8C%EB%A7%81-aka.-Nucleus-Sampling) 
## 26-3_임정훈
### 진행 시각
15:20

### 키워드
#### 모델
- `AutoModelForCausalLM.from_pretrained` 
#### 토크나이저
- skt/kogpt2-base-v2 ( SK Telecom에서 개발한 한국어 gpt2 모델의 토크나이저), bos, eos, unk, pad
- SFT_dataset 정의: `__init__`_ , _`___len___`_, _ _ _ (특정 샘플을 검색하는데 사용)
- `@dataclass`, `DataCollatorForSupervisedDataset` 정의(가져온 데이터를 배치 형태로 조합)
### 퀴즈
**Q9. SFT_dataset 클래스의 이니셜라이저에서 **`**label[:source_len] = -100**`**0 코드의**`**-100**`** 이이 의미하는 게 무엇인가요? 해당 코드가 필요한 이유와 그 기능은 무엇인가요?**

-  -100은 Pytorch에서 특정 토큰의 손실 계산을 무시하도록 지시하는데 사용되는 숫자이다.
- 필요한 이유: 언어 생성 모델은 입력 시퀀스를 받아 다음 토큰을 예측하도록 훈련되지만 입력 시퀀스는 토큰들의 예측 대상이 아니므로 무시해야 한다.
- 기능: label 배열의 처음부터 source_len까지의 토큰들을 -100으로 설정을 해서 모델은 입력 시퀀스의 토큰들에 대해서는 손실을 계산하지 않고 출력 시퀀스의 토큰들에 대해서만 손실을 계산한다.
- 참고 링크: [﻿pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) 


**Q10. **`**DataCollatorForSupervisedDataset**`** 클래스에서 **`**labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)**`** 코드의**
`**padding_value= -100**`** 인자의 **`**-100**`** 은 어떤 기능을 하나요?**

- 레이블의 패딩된 부분이 모델 학습에 영향을 미치지 않도록 한다.


**Q11. 아래 코드블럭에 train_dataset.input_ids[0] 와 train_dataset.labels[0]를 디코딩하는 함수를 작성하여 정수토큰들의 인코딩전 원래 문장이 어떤 형태로 토크나이징 되었는지 확인해보세요.**

```
Decoded input: ### Instruction(명령어):
불고기용 고기 한우에요? 
```
```
Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.
```


**Q12. **[﻿허깅페이스의 Trainer 클래스 다큐먼트](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments) **를 참고해 위 코드블럭의 TrainingArguments의 인자들이 각각 어떤 기능을 하는지 찾아보세요. 그리고 변경 및 추가 가능한 학습옵션에는 어떤 것들이 있는지 살펴보세요.**

- 사용된 인자
`output_dir`: 모델 예측 및 체크포인트를 저장
`overwrite_output_dir`: True이면, output_dir에 overwrite한다.
`num_train_epochs`: 총 훈련 에포크 수.
`per_device_train_batch_size`: 훈련을 위한 장치당 배치 크기.
`per_device_eval_batch_size`: 평가를 위한 장치당 배치 크기.
`warmup_steps`: 초기 학습률까지 선형 준비 단계를 수행하는 단계 수.
`prediction_loss_only`: True이면, 평가 또는 예측 시 손실만 반환.
`fp16`: True이면, 16비트 부동 소수점을 사용하여 mixed precision training을 활성화한다.


- 다른 인자
`valuation_strategy`: 평가 전략을 설정합니다. 예를 들어, epoch로 설정하면 각 에포크의 끝에서 평가를 수행합니다.
`save_strategy`: 모델 체크포인트 저장 전략을 설정합니다. epoch 또는 steps로 설정할 수 있습니다.
`logging_dir`: 로그 파일을 저장할 디렉토리의 경로입니다.
`logging_steps`: 로깅을 수행할 스텝 간격을 설정합니다.
`save_total_limit`: 저장할 체크포인트의 최대 개수를 설정합니다. 이를 초과하면 오래된 체크포인트를 삭제합니다.
`load_best_model_at_end`: True로 설정하면, 학습이 끝난 후 가장 좋은 성능을 보인 모델 체크포인트를 자동으로 로드합니다.
`metric_for_best_model`: 가장 좋은 모델을 선택할 때 사용할 평가 지표를 설정합니다.


**Q13. **[﻿허깅페이스의 Text generation strategies](https://huggingface.co/docs/transformers/v4.28.1/en/generation_strategies) **를 참고해 generation_args에 있는 각 옵션의 기능을 확인하세요. 변경하거나 추가할 수 있는 옵션에는 어떤 것들이 있는지 살펴보세요.**

- `num_beams` 빔 서치에서 사용되는 빔의 수이다. 더 높은 값은 더 많은 가능성을 탐색한다.
- `repetition_penalty`: 반복된 단어나 구문의 사용을 억제한다.
- `no_repeat_ngram_size`: 설정된 n-gram 크기 동안 반복을 방지한다.
- `eos_token_id`: 문장 종료 토큰을 지정.
- `max_new_tokens`: 생성할 새 토큰의 최대 수를 제한한다.
- `do_sample`: 샘플링 방식을 활성화하여 예측 가능성을 다양화한다.
- `top_k`: 최상위 k개의 토큰만 고려하여 선택 범위를 제한한다.
- `early_stopping`: 빔 서치에서 문장 종료 토큰에 도달하면 생성을 조기 종료한다.
추가할 수 있는 옵션

- `temperature`: 생성된 텍스트의 예측 가능성과 다양성 조절절

- `op_p `확률 분포를를 기반으로 다음 토큰을 선택


## 26-4_서민성
### 진행 시각
- 16:00~ Q14-15
### 키워드 
`reward model` 

- 강화학습에서 주로 사용되는 개념으로 학습시 피드백을 제공하여 원하는 행동을 학습하도록 도와주는 모델


### 퀴즈
**Q14.** **SFT 모델의 출력 결과를 Reward model이 받아 어떻게 처리하게 되는지 잠시 생각해봅시다. 여기서 Reward model로 GPT2를 사용하는 이유가 무엇일까요?**

- SFT 모델의 출력 결과는 문자열입니다. 이 문자열이 얼마나 좋은 결과인지를 평가하기 위해서는 다음과 같은 요소들을 고려해야 합니다.
    - 문법적 정확성: 문법적으로 올바른 문장인지를 평가합니다.
    - 의미적 일관성: 의미적으로 일관성 있는 문장인지를 평가합니다.
    - 창의성: 새로운 아이디어를 담고 있는 문장인지를 평가합니다.
- gpt2는 이러한 요소들을 고려하여 문장의 품질을 평가할 수 있는 능력과 학습된 방대한 데이터 역시 갖고 있어서 gpt2를 사용함
- gpt2는 gpt1에 대비하여 큰 파라미터를 보유하고 있고, 자연어 생성에 중점을 둔 모델임. 물론 gpt3를 reward 모델에 사용하는 것이 성능 측면에서 더 좋을 수 있지만, 더 많은 파라미터로 인하여 발생되는 높은 cost를 고려한다면 gpt2가 적절한 것으로 보입니다.


**Q15. **`**nn.Linear()**`** 레이어의 인자로 **`**model.config.n_embd**`**와 **`**1**`**이 주어졌습니다. **`**model.config.n_embd**`**는 무엇이고 **`**1**`**은 왜 입력된 것일까요?**

[﻿파이토치 문서 해당 부분](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear)﻿

- GPT2는 SFT 모델의 출력결과를 다양한 관점(문법의 정확성, 의미적 일관성, 창의성 등)으로 평가가 가능하지만, 이러한 요소들을 개별적으로 평가하지 않고 종합점수를 계산함, 그렇기에 `nn.linear`레이어의 출력차원을 1로 설정하여 단일 스칼라 값으로 점수를 표현하게됨 
- 해당 점수는 좋은 샘플을 생성했는지에 대한 지표로 활용되고 sft 모델은 이 보상점수를 높이기 위해 훈련됨. 
- `model.config.n_embd`는 GPT2 모델의 임베딩 차원을 나타냅니다.
nn.Linear 레이어의 출력 차원은 1로 설정되어 스칼라 값으로 보상 점수를 표현합니다. 이 보상 점수는 SFT 모델의 출력 결과에 대한 "좋음"의 정도를 나타냄.


Q16. 아래** 코드블럭에서 **`**chosen_reward - reject_reward**`** 식은 어떤 연산을 의미하나요?**
`**loss = -log_probs.mean()**`** 코드는 무엇을 최대화하는 연산으로 해석할 수 있을까요?**

```
class PairWiseLoss(nn.Module):
def forward(self, chosen_reward: torch.Tensor, reject_reward: torch.Tensor) -> torch.Tensor:
        probs = torch.sigmoid(chosen_reward - reject_reward)
        log_probs = torch.log(probs)
        loss = -log_probs.mean()
        return loss
```
- `chosen_reward`은 SFT 모델이 선택한 샘플에 대한 보상 점수, `reject_reward`은 SFT 모델이 거절한 샘플에 대한 보상 점수이므로, `chosen_reward - reject_reward` 식은 선택한 샘플이 거절한 샘플보다 얼마나 좋은지를 나타내는 지표
- `log_probs`는 확률을 자연로그로 변환한 값, `mean()`은 평균 연산이므로, `loss = -log_probs.mean()`코드는 다음과 같은 의미를 갖는다.
    - 기본적으로 이 식은 선택한 샘플이 거절한 샘플보다 얼마나 좋은지를 나타내는 확률을 최대화하는 손실 함수다.
    - `chosen_reward - reject_reward` 식은 선택한 샘플이 거절한 샘플보다 얼마나 좋은지를 나타내는 지표
    - 이 지표를 고려할 때, 손실 함수를 최소화하는 것은 확률을 최대화하는 것과 같다. 즉, SFT 모델은 선택한 샘플이 거절한 샘플보다 얼마나 좋은지를 나타내는 확률을 최대화하도록 학습한다.


Q17. **ranking dataset 함수를 수정하게 되면 ranking data가 어떻게 만들어지게 되나요?**
**둘의 차이를 비교하고 어떤 데이터셋을 사용하는게 더 적절한지 이야기해봅시다.**

[기존 코드]

```
total_data_ranking2chosen = []
for tmp in list_data_dict:
    one_data_ranking2chosen = []

    data = {}
    data['prompt'] = tmp['prompt']
    if tmp['ranking'][0] < tmp['ranking'][1]:
        data['chosen'] = tmp['completion_0']
        data['rejected'] = tmp['completion_1']
    else:
        data['chosen'] = tmp['completion_1']
        data['rejected'] = tmp['completion_0']
    one_data_ranking2chosen.append(data)

    data = {}
    data['prompt'] = tmp['prompt']
    if tmp['ranking'][0] < tmp['ranking'][2]:
        data['chosen'] = tmp['completion_0']
        data['rejected'] = tmp['completion_2']
    else:
        data['chosen'] = tmp['completion_2']
        data['rejected'] = tmp['completion_0']
    one_data_ranking2chosen.append(data)

    data = {}
    data['prompt'] = tmp['prompt']
    if tmp['ranking'][1] < tmp['ranking'][2]:
        data['chosen'] = tmp['completion_1']
        data['rejected'] = tmp['completion_2']
    else:
        data['chosen'] = tmp['completion_2']
        data['rejected'] = tmp['completion_1']
    one_data_ranking2chosen.append(data)



    total_data_ranking2chosen.extend(one_data_ranking2chosen)

print('before data num: %d'%(len(list_data_dict)))
print('after  data num: %d'%(len(total_data_ranking2chosen)))
print('data example: \n%s'%total_data_ranking2chosen[45])
```
[수정된 코드]

```
total_data_ranking2chosen = []
for tmp in list_data_dict:
     prompt = tmp['prompt']
     ranking = tmp['ranking'] # ranking = [2,1,0] 
     for index in range(1, len(ranking)): # len(ranking) == 3
         n = ranking[0] # = 2 -> completion_2가 good
         m = ranking[index] # 1(index=1일 때), 0

         data = {
             'prompt': prompt,
             'chosen': tmp['completion_{}'.format(n)],
             'rejected': tmp['completion_{}'.format(m)]
         }
         total_data_ranking2chosen.append(data)
         
```
data_1 = {prompt, chosen = good, rejected = bad}
data_2 = {prompt, chosen = good, rejected = worst}
수정된 코드에서는 위 2개의 데이터가 들어가고
수정 전 코드에서는 추가로 1개가 더 들어간다:
  data_3 = {prompt, chosen=bad, rejected=worst}

```
```


- 이러한 차이점은 다음과 같이 해석할 수 있습니다.
    - 기존의 ranking dataset 함수는 SFT 모델이 선택한 샘플과 거절한 샘플 간의 차이를 평가하는 데 중점을 두었습니다.
    - 수정된 ranking dataset 함수는 SFT 모델이 선택한 샘플의 품질을 평가하는 데 중점을 두었습니다.
- SFT 모델이 선택한 샘플의 품질을 평가하는 데 중점을 두는 경우에는 수정된 ranking dataset 함수가 더 적절합니다. 
    - 예를 들어, SFT 모델을 사용하여 유머러스한 텍스트를 생성하는 경우, 기존의 ranking dataset 함수를 사용하여 선택한 샘플의 품질을 평가하는 것이 더 적절합니다. 왜냐하면, 유머러스한 텍스트는 주관적인 평가에 따라 품질이 달라질 수 있기 때문입니다.
- SFT 모델이 선택한 샘플과 거절한 샘플 간의 차이를 평가하는 데 중점을 두는 경우에는 기존의 ranking dataset 함수가 더 적절합니다.
    - SFT 모델을 사용하여 사실적인 텍스트를 생성하는 경우, 기존의 ranking dataset 함수를 사용하여 선택한 샘플과 거절한 샘플 간의 차이를 평가하는 것이 더 적절합니다. 왜냐하면, 사실적인 텍스트는 주관적인 평가보다는 객관적인 평가에 따라 품질이 달라질 수 있기 때문입니다.


Q18. **RewardDataset 클래스는 어떤 기능을 수행하나요? **`**chatgpt/dataset**`**폴더 내의 **`**reward_dataset**`**모듈을 참고해 답해보세요.**

```
민성님 전언: 이부분 저도 이해를 잘 못햇습니다.
```
<`**RewardDataset**`**클래스가 사용된 코드>**

```
train_data = total_data_ranking2chosen[:1000]
eval_data = total_data_ranking2chosen[1000:1200]

train_dataset = RewardDataset(train_data, tokenizer, 512)
eval_dataset = RewardDataset(eval_data, tokenizer, 512)
```
<`**RewardDataset**`**클래스>**

```

from typing import Callable

from torch.utils.data import Dataset
from tqdm import tqdm

from .utils import is_rank_0


class RewardDataset(Dataset):
    """
    Dataset for reward model

    Args:
        dataset: dataset for reward model
        tokenizer: tokenizer for reward model
        max_length: max length of input
    """

    def __init__(self, dataset, tokenizer: Callable, max_length: int) -> None:
        super().__init__()
        self.chosen = []
        self.reject = []
        for data in tqdm(dataset, disable=not is_rank_0()):
            prompt = data['prompt']

            chosen = prompt + data['chosen'] + "<|endoftext|>"
            chosen_token = tokenizer(chosen,
                                     max_length=max_length,
                                     padding="max_length",
                                     truncation=True,
                                     return_tensors="pt")
            self.chosen.append({
                "input_ids": chosen_token['input_ids'],
                "attention_mask": chosen_token['attention_mask']
            })

            reject = prompt + data['rejected'] + "<|endoftext|>"
            reject_token = tokenizer(reject,
                                     max_length=max_length,
                                     padding="max_length",
                                     truncation=True,
                                     return_tensors="pt")
            self.reject.append({
                "input_ids": reject_token['input_ids'],
                "attention_mask": reject_token['attention_mask']
            })

    def __len__(self):
        length = len(self.chosen)
        return length

    def __getitem__(self, idx):
        return self.chosen[idx]["input_ids"], self.chosen[idx]["attention_mask"], self.reject[idx][
            "input_ids"], self.reject[idx]["attention_mask"]
```
- RewardDataset 클래스는 SFT 모델의 훈련에 사용되는 데이터셋을 생성하는 클래스입니다. 이 클래스는 다음과 같은 기능을 수행합니다.
    - 텍스트 데이터를 읽고 처리합니다.
    - 텍스트 데이터를 선택된 샘플과 거절된 샘플의 쌍으로 구성합니다.
    - 쌍으로 구성된 데이터를 리스트로 반환합니다.
- RewardDataset 클래스는 다음과 같은 두 가지 방법으로 데이터셋을 생성할 수 있습니다.
    - ranking_file 인수를 사용하여 랭킹 파일을 지정합니다. 랭킹 파일은 각 샘플의 선택 순서를 나타내는 파일입니다.
    - completions 인수를 사용하여 완성된 텍스트 리스트와 랭킹을 지정합니다.

    - 랭킹 파일을 사용하여 데이터셋을 생성하는 경우, RewardDataset 클래스는 다음과 같은 단계를 통해 데이터셋을 생성합니다
        - 랭킹 파일을 읽습니다.
        - 각 랭킹에 대해 선택된 샘플과 거절된 샘플을 추출합니다.
        - 선택된 샘플과 거절된 샘플을 쌍으로 구성합니다.

    - 완성된 텍스트 리스트와 랭킹을 사용하여 데이터셋을 생성하는 경우, RewardDataset 클래스는 다음과 같은 단계를 통해 데이터셋을 생성합니다.
        - 완성된 텍스트 리스트를 읽습니다.
        - 랭킹에 따라 완성된 텍스트를 정렬합니다.
        - 각 랭킹에 대해 선택된 샘플과 거절된 샘플을 추출합니다.
선택된 샘플과 거절된 샘플을 쌍으로 구성합니다.
### 참고자료
- [﻿questionet.tistory.com/81](https://questionet.tistory.com/81) 
- [﻿huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf) 
- [﻿tech.scatterlab.co.kr/alt-rlhf/](https://tech.scatterlab.co.kr/alt-rlhf/)  RLHF를 대체하여 feed back을 학습하려던 시도에 대한 글


## 26-5_김지원 (지원님 내용 정리 어려우면 진행만 부탁드립니다)
- 진행 시각: 내일(8일) 시작
- 워드 : 
- 퀴즈
1. 
2. 
- 참고자료


## 26-6_김정현
- 진행 시각:
- 키워드 : 
- 퀴즈
1. 
2. 
- 참고자료


## 27 프로젝트 



<!--- Eraser file: https://app.eraser.io/workspace/aJvSKAFc25fpWQxaIrT7 --->