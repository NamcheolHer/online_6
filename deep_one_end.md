<p><a target="_blank" href="https://app.eraser.io/workspace/Qt07yUo7CrWs1msnek4z" id="edit-in-eraser-github-link"><img alt="Edit in Eraser" src="https://firebasestorage.googleapis.com/v0/b/second-petal-295822.appspot.com/o/images%2Fgithub%2FOpen%20in%20Eraser.svg?alt=media&amp;token=968381c8-a7e7-472a-8ed6-4a6626da5501"></a></p>

딥한끝에서 궁금했던 여기 아카이빙 해볼까요? 다같이 ㄱㄱ



## 노드 01
저희 초창기에 나융퍼실님이 미분불가능하다는 의미가 더 있다고 설명해주셨는데, 구체적인 내용이 기억이 안납니다. ㅠㅠ MLP에서 역전파를 죽이는 것 외에 구체적인 의미가 더 있다고 하셨는데 까먹었습니다.  (태하)

-> 질문이 미분 불가능하다는 것의 의미가 어떤 의미가 있는지에 대한 것인가요? 

-> 미분 불가능 하다는 것의 의미는 알고 있는데, 그거 말고 또 다른 의미를 설명해주셨던 것 같은데, 제가 정확히 기억이 안나서요!!



## 노드 02
batch normalization이 과적합을 방지하는데 왜 유용한지?  (민규)  -> 2222(김산)

[﻿-> ﻿https://dbstndi6316.tistory.com/384](https://dbstndi6316.tistory.com/384)  이거보시면 어느정도 도움될 것 같아요.

-> 정규화를 통해 과적합을 어떻게 억제하나요? 블로그에는 이런 내용이 없는 것 같아요

-> 아하. 저희가 ML을 돌리는 것은 결국 데이터(숫자) 간의 특성(정보)를 추출하고 도출하는 과정이잖아요? 그렇다면, 과적합은 다른 말로 하면 특정 데이터(숫자) 간의 특성에 너무 협소하게 정보를 추출한다라고 이해할 수 있겠죠? -> 그러면 숫자 간의 특성을 줄여주려면, 결국 데이터(숫자) 간 가지고 있는 특성, 통계학적으로는 분산와 평균을 비슷하게 바꿔준다면 -> 그러니까 정규화를 해준다면, 과적합을 해결할 수 있겠죠. 결국 정규화라는 것이 숫자 간의 특성을 유지한 채로 다른 데이터(숫자)와 공정하게(?) 비교할 수 있게 하는 과정이니까요. (제가 틀릴 수도 있긴합니다 ㅋㅋㅋㅋ 저는 이렇겢 이해했어요)

-> 👍

-> 튀는 값을 잡아줌(normalization)으로써 강건하게 학습할 수 있다

배치마다 정규화 파라미터가 달라지는데 이것이 일종의 노이즈로 작용하여 과적합을 방지하는 효과도 있다고 합니다(서연)

-> 👍

## 노드 03
subclass api는 주로 customize 하는데 쓰인다고 나와있는데, customize라는 개념이 결국 다음에 다시 사용할때 편하게 쓰려고 한다는 의미인가요? -> 호재

-> 정확히는 내가 원하는대로 재정의해서 사용하고 싶기 때문이 아닐까요?

0에 가까운 부근에서 exponential 연산을 수행하면, 왜 계산 비용이 많이 드나요? 그래프를 보면서 직관적으로 이해는 얼추되는데, 사례로 조금 더 설명해주실 수 있나요?(태하)

-> 2가지로 나눠보면 , 0부근에서 exp 계산비용이 높은 이유 / exp 계산 자체가 비용이 높은 이유

-> 0부근에서 계산 비용이 높은 이유는 값이 급격히 작아지기 때문에  (메모리, 다루는 비용이 커짐)

-> exp 계산 자체가 비싼 이유 (계산 자체가 복잡함, 값을 정확하게 표현하기 위해서는 정교하게 수를 다뤄야 함 등등 컴퓨터 공학적인 이유 )

-> 0 부근 값 표현을 위해 부동소수점 타입이 사용되면 비트 표현 연산 비용도 추가될 것 같습니다. (청)



layer구성 시 dense에서 유닛(노드)의 숫자가 가지는 의미가 무엇인가요? 구체적으로 덴스 레이어에서 유닛(노드) 수가 많아질 때와 적을 때, 어떤 차이가 있는지 궁금합니다. (태하)

-> 출력되는 값의 개수입니다. 유닛수가 많으면 파라미터 수가 증가하고 데이터를 더 깊게 학습할 수 있을 것 같습니다.

-> 히든 값이 커지면 데이터의 특징을 더 많이 포착할 수 있을 것 같습니다. 다만 데이터 뷴포나 도메인마다 적절한 모델 깊이나 히든 크기가 다를 것 같습니다. (청) 

-> 츌력 노드의 개수 즉, 출력할 차원의 수를 정한다고 볼 수 있습니다. 그래서 데이터가 가지는 값들을 몇 차원으로 표현할지 정하는 부분이기 때문에 어떠한 경우에 어떠한 값을 써야한다라기 보다 실험을 통해 출력 차원을 정해주는 것이 필요하다고 생각합니다.



## 노드 04






## 노드 05
5-3. json으로 모델 저장 후 다시 불러왔을 때 예측이 잘 안 되는데 이유가 뭘까요?

-> json으로 저장시 모델 구조만 저장하고 가중치는 저장이 안 된다고 합니다.



 5-3. 서브클래싱 모델의 경우 가중치만 저장된다고 하는데 그럼 모델 구조는 코드로 저장해야하나요? (청)

-> json 파일로 저장하고 불러온 후에 가중치를 불러오면 되지 않을까요? 

   -> 그럼 서브클래스 모델은 json으로 모델 구조를 저장하고 save_weights()로 가중치를 저장하는 2단계가 필요하겠네요. (청)



## 노드 06
( MK )

데이터 용량이 커지면서 리소스의 한계로 미니배치 방식을  택하게 된거라면, 

리소스의 한계가 없다면 full batch가 좋은걸까요? 

비슷하게 batch size는 클수록 좋을까요? 

남철 : 리소스 한계 측면을 무시한다면, 일반적으로 풀배치가 더 좋습니다. 왜 그럴지 생각해보세요. [﻿**참고**](https://welcome-to-dewy-world.tistory.com/86) 



## 노드 07
유닛수와 레이어수 차이?

복잡한 모델 사용 시 과대적합이 쉽게 될 수 있다고 하는데, 얕은 모델로 변경하는것과 복잡한모델+에폭수 줄이기랑은 선택의 영역인가요?



(MK)

테스트 단게에서의 드랍아웃 작동방식?

( 그 어떤 노드도 드랍아웃 되지 않고, 대신 해당 레이어의 출력 노드를 드랍아웃 비율게 맞게 줄여주게 된다는데, 어떻게 줄여주는걸까?)

남철 : 어디서 나온 설명인가요? 노드와 스탭도 알려주세요. [﻿참고자료](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) 

-> 7-4 노드입니다! [﻿참고자료2](https://wikidocs.net/196901#:~:text=5.%20Dropout%20during%20Inference) 



## 노드 08


- 시그모이드 함수가 (-1, 1) 에서는 선형함수인 이유? (비선형일 때와의 차이)



- Xavier or He 초기화에 대한 의문 
표준편차 값이 작아지면 가중치는 더 0에 뭉쳐서 생기는게 아닌가요??


- 배치 정규화의 위치 (dense layer와 actiation 사이) 
사이 or 뒤 중 어느 곳에 위치하는게 좋을까요?










## 노드 09








## 노드 10



<!--- Eraser file: https://app.eraser.io/workspace/Qt07yUo7CrWs1msnek4z --->