<p><a target="_blank" href="https://app.eraser.io/workspace/ieQMukGjwRNXtyss0S1o" id="edit-in-eraser-github-link"><img alt="Edit in Eraser" src="https://firebasestorage.googleapis.com/v0/b/second-petal-295822.appspot.com/o/images%2Fgithub%2FOpen%20in%20Eraser.svg?alt=media&amp;token=968381c8-a7e7-472a-8ed6-4a6626da5501"></a></p>

- 인코더 디코더
- Q, K, V
- 포지셔널 인코딩 참고자료: [﻿https://gaussian37.github.io/dl-concept-positional_encoding/](https://gaussian37.github.io/dl-concept-positional_encoding/)  
- 왜 GPU로 병렬 연산을 하면 빠르게 계산 가능한가?
- 엔비디아 주식 지금 이 살때다? US$468.06
- CPU vs GPU




### 피드백 공유해봐요.
공통 아래 질문을 생각해봅시다.

- LOSS FUNC 설정시 정답셋과 같은 의미라면 정답으로 처리하면 좋겠다는 의견을 제안하셨네요. **정답셋과 같은 의미라는 것**을 어떻게 알수 있을까요? 본프로젝트에서 LOSS FUNC은 어떻게 계산되고 있나요?
    - SparseCategoricalCrossentropy vs  CategoricalCrossentropy
- 일정 에폭마다 결과를 비교하고자 하였다면, 모델 학습설계 단계에서 10에폭 마다 모델을 저장하고, 저장한 모델별로 인퍼런스를 불러올수 있도록 뼈대 코드를 작성했다면 좋았을 것이에요.
    - 당연히 모델 에폭 마다 저장할 모델명에 에폭값을 붙여서 저장해야겠죠?
- 불용어 처리에 대한 제안을 해주셨네요. 불용어 처리 과정을 추가한다면, 전처리과정에서 어느 단계에 들어가야 할까요? 또 어떤 효과를 가져 올까요?
    - 
- 더 다양한 인퍼런스로 결과를 확인하고 비교해보세요. 반복적으로 사용할 인퍼런스 입력을 함수화 하면 그 결과를 확인하기 더욱 편하지 않을까요?
    - 
- 학습데이터가 부족하다는 의견이 있습니다. 데이터를 어떻게 하면 더 추가할수 있을까? [﻿https://aihub.or.kr/](https://aihub.or.kr/)  는 대표적인 무료로 데이터셋을 구할수 있는 곳입니다. 어떤 데이터들이 있는지 어떻게 신청하고 사용할수 있는지, 각 데이터는 어떻구조인지 확인해보세요. 나아가 데이터셋을 추가하여 학습해 보아도 좋을거에요.
- 직접 수집. 크롤링 스크래핑.
- [﻿김태민](https://github.com/ktm379/AIFFEL_Online_6th/blob/main/Exploration/Ex_08/16_1_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8_%ED%95%9C%EA%B5%AD%EC%96%B4_%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C_%EC%B1%97%EB%B4%87_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb) : 각 에폭에서 결과를 한눈에 들어오게 잘 표현
- [﻿김민규](https://github.com/mkk4726/Exploartion/tree/main/EX_8) : 개념을 시각화 한 자료



<!--- Eraser file: https://app.eraser.io/workspace/ieQMukGjwRNXtyss0S1o --->